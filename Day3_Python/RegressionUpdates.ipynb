{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression, Part 2\n",
    "\n",
    "In this notebook we continue our exploration of regression. We introduce using [folds](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html) as well as more complex regression models where we basically come up with different measures of optimal, as we discussed in the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import DS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation using kfold and cross-validation on two regression datasets\n",
    "\n",
    "Here is the code we are going to use here.\n",
    "\n",
    "```Python\n",
    "def runBestRegressionModelKFold(dataSets=[],regModels=[],names=[]):\n",
    "\n",
    "    myResults={}\n",
    "    for ds in dataSets:\n",
    "        myData,myTrain,myVal=dataEncoding(ds,taskID='filesReg')\n",
    "        for name in myTrain.columns:\n",
    "          if (not(myTrain[name].dtype=='O')):\n",
    "            myTrain[name]=pre.minmax_scale(myTrain[name].astype('float')) \n",
    "        #myTrain = skb(f_regression, k=3).fit_transform(myTrain,myVal)\n",
    "        splits =kf(n_splits=10, shuffle=True, random_state=42)\n",
    "        infinity = float(\"inf\")\n",
    "        index=-1 \n",
    "        count =-1\n",
    "        for reg in regModels:\n",
    "            count = count +1\n",
    "            reg.fit(myTrain, myVal)\n",
    "            cvsScores=cvs(reg, myTrain, myVal,cv=splits,scoring='neg_mean_squared_error')\n",
    "            meanSquareRootError=np.sqrt(-1*cvsScores.mean())\n",
    "            print(regsNames[names[count]],meanSquareRootError)\n",
    "            if (meanSquareRootError < infinity):\n",
    "                infinity = meanSquareRootError\n",
    "                index = count\n",
    "                L1,L2,L3,L4,L5,L6= regsNames[names[index]],reg.intercept_,reg.coef_, np.exp(reg.coef_), cvsScores, infinity\n",
    "        print(filesReg[ds],regsNames[names[index]],infinity)\n",
    "        myResults[filesReg[ds]]={1:L1,2:L2,3:L3,4:L4,5:L5,6:L6}\n",
    "        print('\\n')     \n",
    "    return myResults    \n",
    "```\n",
    "\n",
    "### What does the code do?\n",
    "\n",
    "This function runs a complete work flow. We pass in a list of data sets (`dataSets`) and regression models. We run **nested** for loops.\n",
    "\n",
    "* For **each** data set we will compute **each** regression model.\n",
    "\n",
    "Each time we read in a data set we\n",
    "\n",
    "1. transform the data with the `minmax_scale`\n",
    "1. Split the data into 10 folds\n",
    "\n",
    "For each regression model we\n",
    "\n",
    "1. Use cross validation with our splits (k-folds)\n",
    "1. Compute the mean error across all of our folds.\n",
    "\n",
    "#### These are the scikit-learn functions/methods we use\n",
    "\n",
    "* [KFold](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html): \"Split dataset into k consecutive folds (without shuffling by default)\"\n",
    "* [minmax_scale](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.minmax_scale.html): \"Transforms features by scaling each feature to a given range\"\n",
    "* [cross_val_score](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html): \n",
    "* [\\*.fit](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression.fit): Fit the model.\n",
    "\n",
    "### What does the function return?\n",
    "\n",
    "The function returns a dictionary (keyed by data name) with values of dictionaries (keyed by integers 1-6)\n",
    "\n",
    "DS.runBestRegressionModelKFold:[a relative link](DS.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearRegression 54.9877879922\n",
      "diabetes LinearRegression 54.9877879922\n",
      "\n",
      "\n",
      "LinearRegression 3.26493752482\n",
      "parkinsons LinearRegression 3.26493752482\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "my_results = DS.runBestRegressionModelKFold([0,1],[DS.Regs[0]],[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'diabetes': {1: 'LinearRegression',\n",
       "  2: -2.0761196515562972,\n",
       "  3: array([  -2.18167345,  -22.85964809,  135.59168262,   79.29336753,\n",
       "         -222.35925215,  149.88725147,   28.64436306,   46.32486843,\n",
       "          195.10157471,   18.4877213 ]),\n",
       "  4: array([  1.12852519e-01,   1.18081259e-10,   7.70405821e+58,\n",
       "           2.73320350e+34,   2.69527940e-97,   1.24510570e+65,\n",
       "           2.75479201e+12,   1.31411911e+20,   5.38936141e+84,\n",
       "           1.06933884e+08]),\n",
       "  5: array([-2743.91123776, -2979.19128064, -2427.07375419, -2943.53472742,\n",
       "         -3649.95462856, -2941.17849061, -2408.5223554 , -3170.73664908,\n",
       "         -3485.68036143, -3486.78479771]),\n",
       "  6: 54.987787992237692},\n",
       " 'parkinsons': {1: 'LinearRegression',\n",
       "  2: 9.856898784862377,\n",
       "  3: array([  3.32407981e+00,  -1.37440629e+00,   5.57894794e-01,\n",
       "          -3.06185410e+01,   6.89111466e+00,   3.76175496e+02,\n",
       "           3.98898308e+00,  -3.53357341e+02,  -7.80868096e+00,\n",
       "          -1.39471115e+00,  -2.19009877e+03,   1.47102248e+01,\n",
       "          -9.84354620e+00,   2.18740147e+03,  -1.90117432e+00,\n",
       "          -3.81456728e+00,   2.66469805e+00,  -9.21614724e-01,\n",
       "          -2.78797805e+00,   4.22555533e+01]),\n",
       "  4: array([  2.77734301e+001,   2.52989755e-001,   1.74699085e+000,\n",
       "           5.04123108e-014,   9.83497068e+002,   2.34931899e+163,\n",
       "           5.39999479e+001,   3.45825279e-154,   4.06193479e-004,\n",
       "           2.47904634e-001,   0.00000000e+000,   2.44663655e+006,\n",
       "           5.30887135e-005,               inf,   1.49393081e-001,\n",
       "           2.20472527e-002,   1.43636117e+001,   3.97876062e-001,\n",
       "           6.15455303e-002,   2.24570973e+018]),\n",
       "  5: array([ -9.31886022, -10.62722593, -11.44172269, -10.63916458,\n",
       "         -10.96116743, -10.66133155,  -9.85261988, -10.29783471,\n",
       "         -11.31909945, -11.47914396]),\n",
       "  6: 3.2649375248185764}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('diabetes', 1) LinearRegression\n",
      "('diabetes', 2) -2.07611965156\n",
      "('diabetes', 3) [  -2.18167345  -22.85964809  135.59168262   79.29336753 -222.35925215\n",
      "  149.88725147   28.64436306   46.32486843  195.10157471   18.4877213 ]\n",
      "('diabetes', 4) [  1.12852519e-01   1.18081259e-10   7.70405821e+58   2.73320350e+34\n",
      "   2.69527940e-97   1.24510570e+65   2.75479201e+12   1.31411911e+20\n",
      "   5.38936141e+84   1.06933884e+08]\n",
      "('diabetes', 5) [-2743.91123776 -2979.19128064 -2427.07375419 -2943.53472742 -3649.95462856\n",
      " -2941.17849061 -2408.5223554  -3170.73664908 -3485.68036143 -3486.78479771]\n",
      "('diabetes', 6) 54.9877879922\n",
      "('parkinsons', 1) LinearRegression\n",
      "('parkinsons', 2) 9.85689878486\n",
      "('parkinsons', 3) [  3.32407981e+00  -1.37440629e+00   5.57894794e-01  -3.06185410e+01\n",
      "   6.89111466e+00   3.76175496e+02   3.98898308e+00  -3.53357341e+02\n",
      "  -7.80868096e+00  -1.39471115e+00  -2.19009877e+03   1.47102248e+01\n",
      "  -9.84354620e+00   2.18740147e+03  -1.90117432e+00  -3.81456728e+00\n",
      "   2.66469805e+00  -9.21614724e-01  -2.78797805e+00   4.22555533e+01]\n",
      "('parkinsons', 4) [  2.77734301e+001   2.52989755e-001   1.74699085e+000   5.04123108e-014\n",
      "   9.83497068e+002   2.34931899e+163   5.39999479e+001   3.45825279e-154\n",
      "   4.06193479e-004   2.47904634e-001   0.00000000e+000   2.44663655e+006\n",
      "   5.30887135e-005               inf   1.49393081e-001   2.20472527e-002\n",
      "   1.43636117e+001   3.97876062e-001   6.15455303e-002   2.24570973e+018]\n",
      "('parkinsons', 5) [ -9.31886022 -10.62722593 -11.44172269 -10.63916458 -10.96116743\n",
      " -10.66133155  -9.85261988 -10.29783471 -11.31909945 -11.47914396]\n",
      "('parkinsons', 6) 3.26493752482\n"
     ]
    }
   ],
   "source": [
    "for key1 in my_results.keys():\n",
    "    for key2 in my_results[key1].keys():\n",
    "        print((key1,key2), my_results[key1][key2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Feature selection](https://en.wikipedia.org/wiki/Feature_selection)\n",
    "\n",
    "### What are the best predictors (independent variables) to use in our model? \n",
    "\n",
    "This is one of the key questions in machine learning. We might rely on domain experts to tell us what variables to use. But as the number of variables increase it is unlikely that experts will understand the relative value of all of them. In our diabetes data set, presumably all the variables that were put in the data were acquired because an expert felt they would be valuable. Yet when we created our model we found that some variables did not play well together\n",
    "\n",
    "```\n",
    "[2] The condition number is large, 1.79e+03. This might indicate that there are\n",
    "strong multicollinearity or other numerical problems.\n",
    "```\n",
    "\n",
    "DS.runBestRegressionModelKFoldwFS:[a relative link](DS.py)\n",
    "\n",
    "### Questions\n",
    "\n",
    "1. **Suitable number of features:** What is the best of number features to use in our model\n",
    "1. What is the relationship between feature selection and statistical hypothesis testing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearRegression 55.8311838339\n",
      "diabetes LinearRegression 55.8311838339\n",
      "\n",
      "\n",
      "LinearRegression 3.35850984206\n",
      "parkinsons LinearRegression 3.35850984206\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'diabetes': {1: 'LinearRegression',\n",
       "  2: -250.95406053073677,\n",
       "  3: array([  6.04202222,   0.92475415,  -1.00602696,  -5.07395034,  48.89461151]),\n",
       "  4: array([  4.20743012e+02,   2.52124833e+00,   3.65668915e-01,\n",
       "           6.25765142e-03,   1.71656390e+21]),\n",
       "  5: array([-2588.16642204, -3228.12897079, -2419.44175056, -3395.63182348,\n",
       "         -3532.32690081, -3354.04018219, -2507.14250101, -3206.19359587,\n",
       "         -3454.05600251, -3486.08273369]),\n",
       "  6: 55.83118383389786},\n",
       " 'parkinsons': {1: 'LinearRegression',\n",
       "  2: -3.3605702161113591,\n",
       "  3: array([  6.63606374e-02,  -1.70012537e-03,   5.14678908e+00,\n",
       "          -3.46218507e+00,   1.22506597e+00]),\n",
       "  4: array([  1.06861203e+00,   9.98301319e-01,   1.71878714e+02,\n",
       "           3.13611609e-02,   3.40439066e+00]),\n",
       "  5: array([ -9.93573537, -11.21597172, -12.18567326, -10.90814118,\n",
       "         -11.40931396, -11.56303156, -10.6267876 , -10.86767993,\n",
       "         -11.97347384, -12.11007516]),\n",
       "  6: 3.3585098420620962}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DS.runBestRegressionModelKFoldwFS([0,1],[DS.Regs[0]],[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Validation using kfold and check the error per fold using diabetes dataset. **\n",
    "\n",
    "DS.runBestRegressionModelKFoldPrintFolderErrors:[a relative link](DS.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def runBestRegressionModelKFoldPrintFolderErrors(dataSets=[],regModels=[],names=[]):\n",
    "\n",
    "    myResults={}\n",
    "    for ds in dataSets:\n",
    "        myData,myTrain,myVal=dataEncoding(ds,taskID='filesReg')\n",
    "        splits =kf(n_splits=2, shuffle=True, random_state=42)\n",
    "        infinity = float(\"inf\")\n",
    "        index=-1 \n",
    "        count =-1\n",
    "        for reg in regModels:\n",
    "            count = count+1\n",
    "            xval_err = 0\n",
    "            for train,test in splits.split(myTrain):\n",
    "                reg.fit(myTrain.ix[train],myVal.ix[train])\n",
    "                p = reg.predict(myTrain.ix[test])\n",
    "                e = p-myVal.ix[test]\n",
    "                print (e)\n",
    "                xval_err += np.dot(e,e)\n",
    "            rmse_10cv = np.sqrt(xval_err/len(myTrain))\n",
    "            print(rmse_10cv)\n",
    "            input(\"Press any key\")\n",
    "            if (rmse_10cv < infinity):\n",
    "                infinity = rmse_10cv\n",
    "                index = count\n",
    "                L1,L2,L3,L4,L5= regsNames[names[index]],reg.intercept_,reg.coef_, np.exp(reg.coef_), infinity\n",
    "        print(filesReg[ds],regsNames[names[index]],infinity)\n",
    "        myResults[filesReg[ds]]={1:L1,2:L2,3:L3,4:L4,5:L5}\n",
    "        print('\\n')     \n",
    "    return myResults\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       51.874387\n",
      "2       29.784233\n",
      "3      -41.222417\n",
      "5       11.551659\n",
      "6      -64.877094\n",
      "7       62.287577\n",
      "9     -102.725416\n",
      "10      -8.712643\n",
      "11      37.359077\n",
      "15      11.974796\n",
      "16      51.257121\n",
      "17      31.635977\n",
      "18      48.895830\n",
      "19     -46.179513\n",
      "22      56.096208\n",
      "24      -8.372552\n",
      "25     -61.484976\n",
      "26     -42.406517\n",
      "29    -111.339785\n",
      "30      39.846025\n",
      "31       2.840608\n",
      "33      27.601882\n",
      "36     -60.660853\n",
      "38      -3.669952\n",
      "39      37.691338\n",
      "42      81.416677\n",
      "45      66.745646\n",
      "46     -50.322612\n",
      "55     -44.334540\n",
      "56     153.080229\n",
      "          ...    \n",
      "395    -88.663330\n",
      "397    -31.115494\n",
      "398    -80.988272\n",
      "399    -50.909816\n",
      "400     34.980971\n",
      "401     -8.702057\n",
      "402    -34.669844\n",
      "407     44.441914\n",
      "408      2.316037\n",
      "409     -4.807604\n",
      "410    -65.769389\n",
      "411     20.188607\n",
      "412    -17.401838\n",
      "414     24.234676\n",
      "415     -2.231255\n",
      "416    -10.050522\n",
      "417    107.216933\n",
      "418     19.778457\n",
      "419     38.263006\n",
      "421    -25.312113\n",
      "422    -21.907016\n",
      "423     58.233883\n",
      "427     66.572686\n",
      "429     10.452257\n",
      "432     44.710817\n",
      "433     -0.589607\n",
      "436      7.770374\n",
      "437      7.986984\n",
      "439    -13.097360\n",
      "440     -7.984125\n",
      "Name: QMDP1Y, dtype: float64\n",
      "1      -12.049623\n",
      "4       -7.387351\n",
      "8       48.343397\n",
      "12     -63.417789\n",
      "13     -15.394367\n",
      "14     -17.420449\n",
      "20      59.902766\n",
      "21      28.685456\n",
      "23      23.750238\n",
      "27      88.031574\n",
      "28      12.694472\n",
      "32     -97.812852\n",
      "34      20.897057\n",
      "35     -13.551790\n",
      "37    -115.183102\n",
      "40      54.668028\n",
      "41      20.595863\n",
      "43     -17.077678\n",
      "44     -40.243851\n",
      "47     -22.251439\n",
      "48       0.477230\n",
      "49      46.530781\n",
      "50       7.411344\n",
      "51     -52.673461\n",
      "52      68.762911\n",
      "53      46.027088\n",
      "54     -54.661304\n",
      "58     -73.259722\n",
      "61      38.365078\n",
      "62      -1.412611\n",
      "          ...    \n",
      "371     18.506286\n",
      "372     36.377926\n",
      "373    -30.029205\n",
      "375    -38.245768\n",
      "377    -66.903188\n",
      "379     98.163860\n",
      "382    134.131864\n",
      "383     31.172115\n",
      "384     49.568356\n",
      "385    -78.125253\n",
      "387   -117.387439\n",
      "392    -24.434355\n",
      "394      6.048345\n",
      "396      9.380939\n",
      "403    -24.377949\n",
      "404   -103.258819\n",
      "405     -1.478555\n",
      "406    -17.602899\n",
      "413      1.618576\n",
      "420     -3.293533\n",
      "424     63.466463\n",
      "425    -34.657113\n",
      "426     47.741421\n",
      "428    -55.169715\n",
      "430    -60.741016\n",
      "431     60.683141\n",
      "434     79.491483\n",
      "435     57.358563\n",
      "438     -0.217281\n",
      "441    -12.462762\n",
      "Name: QMDP1Y, dtype: float64\n",
      "55.1149023997\n",
      "Press any key\n",
      "diabetes LinearRegression 55.1149023997\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'diabetes': {1: 'LinearRegression',\n",
       "  2: -282.87694402138419,\n",
       "  3: array([  0.20030102, -23.10866247,   4.40112721,   0.95599474,\n",
       "          -0.76310553,   0.48291796,  -0.46846236,   0.43246722,\n",
       "          73.10107625,   0.25937708]),\n",
       "  4: array([  1.22177048e+00,   9.20524615e-11,   8.15427324e+01,\n",
       "           2.60125686e+00,   4.66216327e-01,   1.62079693e+00,\n",
       "           6.25964035e-01,   1.54105496e+00,   5.58977124e+31,\n",
       "           1.29612245e+00]),\n",
       "  5: 55.114902399662412}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DS.runBestRegressionModelKFoldPrintFolderErrors([0],[DS.Regs[0]],[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Ridge Regression](https://en.wikipedia.org/wiki/Tikhonov_regularization)\n",
    "\n",
    "Width [Ridge regression](http://scikit-learn.org/stable/modules/linear_model.html#ridge-regression) we take our standard mean square error (MSE) and add a [regularization](https://goo.gl/AbyCf7) term. According to Wikipedia\n",
    "\n",
    ">Regularization, in mathematics and statistics and particularly in the fields of machine learning and inverse problems, is a process of introducing additional information in order to solve an ill-posed problem or to prevent overfitting.\n",
    "\n",
    "In Ridge regression our normalization terms is the $L^2$ norm of our coefficients scaled by the [hyperparameter](https://goo.gl/4nuUSF) $\\alpha$.\n",
    "\n",
    "\\begin{equation}\n",
    "J(\\Theta) = \\text{MSE}(\\Theta) + \\alpha\\frac{1}{2} \\sum_{i=1}^n\\Theta_i^2\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "With hyperparameters, we fix these value when we are optimizing for our $\\Theta_i$ values. We can optimize $\\alpha$ with a separate cross-validation experiment (keeping $\\Theta_i$'s fixed) prior to our cross-validation experiment to optimize $\\Theta_i$.\n",
    "\n",
    "The regression task is now to find the linear function that **minimizes** $J(\\Theta)$---that is, minimizing both the mean square error and the regularization term.\n",
    "\n",
    "### What do you imagine the effect of the regularization term would be?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge Regression with scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class Ridge in module sklearn.linear_model.ridge:\n",
      "\n",
      "class Ridge(_BaseRidge, sklearn.base.RegressorMixin)\n",
      " |  Linear least squares with l2 regularization.\n",
      " |  \n",
      " |  This model solves a regression model where the loss function is\n",
      " |  the linear least squares function and regularization is given by\n",
      " |  the l2-norm. Also known as Ridge Regression or Tikhonov regularization.\n",
      " |  This estimator has built-in support for multi-variate regression\n",
      " |  (i.e., when y is a 2d-array of shape [n_samples, n_targets]).\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <ridge_regression>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  alpha : {float, array-like}, shape (n_targets)\n",
      " |      Regularization strength; must be a positive float. Regularization\n",
      " |      improves the conditioning of the problem and reduces the variance of\n",
      " |      the estimates. Larger values specify stronger regularization.\n",
      " |      Alpha corresponds to ``C^-1`` in other linear models such as \n",
      " |      LogisticRegression or LinearSVC. If an array is passed, penalties are\n",
      " |      assumed to be specific to the targets. Hence they must correspond in\n",
      " |      number.\n",
      " |  \n",
      " |  copy_X : boolean, optional, default True\n",
      " |      If True, X will be copied; else, it may be overwritten.\n",
      " |  \n",
      " |  fit_intercept : boolean\n",
      " |      Whether to calculate the intercept for this model. If set\n",
      " |      to false, no intercept will be used in calculations\n",
      " |      (e.g. data is expected to be already centered).\n",
      " |  \n",
      " |  max_iter : int, optional\n",
      " |      Maximum number of iterations for conjugate gradient solver.\n",
      " |      For 'sparse_cg' and 'lsqr' solvers, the default value is determined\n",
      " |      by scipy.sparse.linalg. For 'sag' solver, the default value is 1000.\n",
      " |  \n",
      " |  normalize : boolean, optional, default False\n",
      " |      If True, the regressors X will be normalized before regression.\n",
      " |      This parameter is ignored when `fit_intercept` is set to False.\n",
      " |      When the regressors are normalized, note that this makes the\n",
      " |      hyperparameters learnt more robust and almost independent of the number\n",
      " |      of samples. The same property is not valid for standardized data.\n",
      " |      However, if you wish to standardize, please use\n",
      " |      `preprocessing.StandardScaler` before calling `fit` on an estimator\n",
      " |      with `normalize=False`.\n",
      " |  \n",
      " |  solver : {'auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag'}\n",
      " |      Solver to use in the computational routines:\n",
      " |  \n",
      " |      - 'auto' chooses the solver automatically based on the type of data.\n",
      " |  \n",
      " |      - 'svd' uses a Singular Value Decomposition of X to compute the Ridge\n",
      " |        coefficients. More stable for singular matrices than\n",
      " |        'cholesky'.\n",
      " |  \n",
      " |      - 'cholesky' uses the standard scipy.linalg.solve function to\n",
      " |        obtain a closed-form solution.\n",
      " |  \n",
      " |      - 'sparse_cg' uses the conjugate gradient solver as found in\n",
      " |        scipy.sparse.linalg.cg. As an iterative algorithm, this solver is\n",
      " |        more appropriate than 'cholesky' for large-scale data\n",
      " |        (possibility to set `tol` and `max_iter`).\n",
      " |  \n",
      " |      - 'lsqr' uses the dedicated regularized least-squares routine\n",
      " |        scipy.sparse.linalg.lsqr. It is the fastest but may not be available\n",
      " |        in old scipy versions. It also uses an iterative procedure.\n",
      " |  \n",
      " |      - 'sag' uses a Stochastic Average Gradient descent. It also uses an\n",
      " |        iterative procedure, and is often faster than other solvers when\n",
      " |        both n_samples and n_features are large. Note that 'sag' fast\n",
      " |        convergence is only guaranteed on features with approximately the\n",
      " |        same scale. You can preprocess the data with a scaler from\n",
      " |        sklearn.preprocessing.\n",
      " |  \n",
      " |      All last four solvers support both dense and sparse data. However,\n",
      " |      only 'sag' supports sparse input when `fit_intercept` is True.\n",
      " |  \n",
      " |      .. versionadded:: 0.17\n",
      " |         Stochastic Average Gradient descent solver.\n",
      " |  \n",
      " |  tol : float\n",
      " |      Precision of the solution.\n",
      " |  \n",
      " |  random_state : int seed, RandomState instance, or None (default)\n",
      " |      The seed of the pseudo random number generator to use when\n",
      " |      shuffling the data. Used only in 'sag' solver.\n",
      " |  \n",
      " |      .. versionadded:: 0.17\n",
      " |         *random_state* to support Stochastic Average Gradient.\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  coef_ : array, shape (n_features,) or (n_targets, n_features)\n",
      " |      Weight vector(s).\n",
      " |  \n",
      " |  intercept_ : float | array, shape = (n_targets,)\n",
      " |      Independent term in decision function. Set to 0.0 if\n",
      " |      ``fit_intercept = False``.\n",
      " |  \n",
      " |  n_iter_ : array or None, shape (n_targets,)\n",
      " |      Actual number of iterations for each target. Available only for\n",
      " |      sag and lsqr solvers. Other solvers will return None.\n",
      " |  \n",
      " |      .. versionadded:: 0.17\n",
      " |  \n",
      " |  See also\n",
      " |  --------\n",
      " |  RidgeClassifier, RidgeCV, :class:`sklearn.kernel_ridge.KernelRidge`\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> from sklearn.linear_model import Ridge\n",
      " |  >>> import numpy as np\n",
      " |  >>> n_samples, n_features = 10, 5\n",
      " |  >>> np.random.seed(0)\n",
      " |  >>> y = np.random.randn(n_samples)\n",
      " |  >>> X = np.random.randn(n_samples, n_features)\n",
      " |  >>> clf = Ridge(alpha=1.0)\n",
      " |  >>> clf.fit(X, y) # doctest: +NORMALIZE_WHITESPACE\n",
      " |  Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      " |        normalize=False, random_state=None, solver='auto', tol=0.001)\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      Ridge\n",
      " |      _BaseRidge\n",
      " |      abc.NewBase\n",
      " |      sklearn.linear_model.base.LinearModel\n",
      " |      abc.NewBase\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      sklearn.base.RegressorMixin\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, alpha=1.0, fit_intercept=True, normalize=False, copy_X=True, max_iter=None, tol=0.001, solver='auto', random_state=None)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  fit(self, X, y, sample_weight=None)\n",
      " |      Fit Ridge regression model\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n",
      " |          Training data\n",
      " |      \n",
      " |      y : array-like, shape = [n_samples] or [n_samples, n_targets]\n",
      " |          Target values\n",
      " |      \n",
      " |      sample_weight : float or numpy array of shape [n_samples]\n",
      " |          Individual weights for each sample\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : returns an instance of self.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.linear_model.base.LinearModel:\n",
      " |  \n",
      " |  decision_function(*args, **kwargs)\n",
      " |      DEPRECATED:  and will be removed in 0.19.\n",
      " |      \n",
      " |      Decision function of the linear model.\n",
      " |      \n",
      " |              Parameters\n",
      " |              ----------\n",
      " |              X : {array-like, sparse matrix}, shape = (n_samples, n_features)\n",
      " |                  Samples.\n",
      " |      \n",
      " |              Returns\n",
      " |              -------\n",
      " |              C : array, shape = (n_samples,)\n",
      " |                  Returns predicted values.\n",
      " |  \n",
      " |  predict(self, X)\n",
      " |      Predict using the linear model\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix}, shape = (n_samples, n_features)\n",
      " |          Samples.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      C : array, shape = (n_samples,)\n",
      " |          Returns predicted values.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : boolean, optional\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : mapping of string to any\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as pipelines). The latter have parameters of the form\n",
      " |      ``<component>__<parameter>`` so that it's possible to update each\n",
      " |      component of a nested object.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.RegressorMixin:\n",
      " |  \n",
      " |  score(self, X, y, sample_weight=None)\n",
      " |      Returns the coefficient of determination R^2 of the prediction.\n",
      " |      \n",
      " |      The coefficient R^2 is defined as (1 - u/v), where u is the regression\n",
      " |      sum of squares ((y_true - y_pred) ** 2).sum() and v is the residual\n",
      " |      sum of squares ((y_true - y_true.mean()) ** 2).sum().\n",
      " |      Best possible score is 1.0 and it can be negative (because the\n",
      " |      model can be arbitrarily worse). A constant model that always\n",
      " |      predicts the expected value of y, disregarding the input features,\n",
      " |      would get a R^2 score of 0.0.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape = (n_samples, n_features)\n",
      " |          Test samples.\n",
      " |      \n",
      " |      y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n",
      " |          True values for X.\n",
      " |      \n",
      " |      sample_weight : array-like, shape = [n_samples], optional\n",
      " |          Sample weights.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          R^2 of self.predict(X) wrt. y.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "help(linear_model.Ridge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How does our choice of $\\alpha$ effect our results?\n",
    "\n",
    "#### Plot different aplhas and show the values of MSE using diabetes\n",
    "\n",
    "DS.runRidgeRegressiontoEstAlpha:[a relative link](DS.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge Regression\n",
      "alpha\t RMSE_train\t RMSE_10cv\n",
      "\n",
      "0.010\t 53.4761\t\t 54.9810\n",
      "0.418\t 53.4775\t\t 54.9782\n",
      "0.826\t 53.4812\t\t 54.9786\n",
      "1.234\t 53.4868\t\t 54.9815\n",
      "1.642\t 53.4939\t\t 54.9864\n",
      "2.050\t 53.5022\t\t 54.9929\n",
      "2.458\t 53.5116\t\t 55.0007\n",
      "2.866\t 53.5217\t\t 55.0096\n",
      "3.274\t 53.5325\t\t 55.0193\n",
      "3.682\t 53.5439\t\t 55.0296\n",
      "4.090\t 53.5556\t\t 55.0404\n",
      "4.498\t 53.5676\t\t 55.0516\n",
      "4.906\t 53.5798\t\t 55.0630\n",
      "5.313\t 53.5922\t\t 55.0747\n",
      "5.721\t 53.6046\t\t 55.0864\n",
      "6.129\t 53.6172\t\t 55.0982\n",
      "6.537\t 53.6297\t\t 55.1101\n",
      "6.945\t 53.6422\t\t 55.1219\n",
      "7.353\t 53.6547\t\t 55.1336\n",
      "7.761\t 53.6670\t\t 55.1453\n",
      "8.169\t 53.6793\t\t 55.1569\n",
      "8.577\t 53.6915\t\t 55.1683\n",
      "8.985\t 53.7036\t\t 55.1797\n",
      "9.393\t 53.7155\t\t 55.1909\n",
      "9.801\t 53.7273\t\t 55.2019\n",
      "10.209\t 53.7390\t\t 55.2128\n",
      "10.617\t 53.7505\t\t 55.2236\n",
      "11.025\t 53.7619\t\t 55.2341\n",
      "11.433\t 53.7731\t\t 55.2445\n",
      "11.841\t 53.7842\t\t 55.2548\n",
      "12.249\t 53.7951\t\t 55.2649\n",
      "12.657\t 53.8058\t\t 55.2748\n",
      "13.065\t 53.8164\t\t 55.2846\n",
      "13.473\t 53.8268\t\t 55.2941\n",
      "13.881\t 53.8371\t\t 55.3036\n",
      "14.289\t 53.8472\t\t 55.3129\n",
      "14.697\t 53.8572\t\t 55.3220\n",
      "15.104\t 53.8670\t\t 55.3310\n",
      "15.512\t 53.8767\t\t 55.3398\n",
      "15.920\t 53.8863\t\t 55.3484\n",
      "16.328\t 53.8956\t\t 55.3570\n",
      "16.736\t 53.9049\t\t 55.3654\n",
      "17.144\t 53.9140\t\t 55.3736\n",
      "17.552\t 53.9230\t\t 55.3817\n",
      "17.960\t 53.9318\t\t 55.3897\n",
      "18.368\t 53.9405\t\t 55.3976\n",
      "18.776\t 53.9491\t\t 55.4053\n",
      "19.184\t 53.9576\t\t 55.4129\n",
      "19.592\t 53.9659\t\t 55.4204\n",
      "20.000\t 53.9742\t\t 55.4277\n",
      "Press Any Key\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZIAAAEKCAYAAAA4t9PUAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f64ea127f28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import DS\n",
    "DS.runRidgeRegressiontoEstAlpha([0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Validation using kfold and cross-validation for alpha and the data using diabetes dataset**\n",
    "\n",
    "DS.runBestRegressionModelKFold:[a relative link](DS.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def runRidgeRegressiontoEstAlpha(dataSets=[]):\n",
    "\n",
    "    from sklearn.linear_model import Ridge\n",
    "    \n",
    "    \n",
    "    print('Ridge Regression')\n",
    "    print('alpha\\t RMSE_train\\t RMSE_10cv\\n')\n",
    "    alpha = np.linspace(.01,20,50)\n",
    "\n",
    "    \n",
    "    for ds in dataSets:\n",
    "        myData,myTrain,myVal=dataEncoding(ds,taskID='filesReg')\n",
    "        #for name in myTrain.columns:\n",
    "          #if (not(myTrain[name].dtype=='O')):\n",
    "            #myTrain[name]=pre.minmax_scale(myTrain[name].astype('float')) \n",
    "        t_rmse = np.array([])\n",
    "        cv_rmse = np.array([])\n",
    "        \n",
    "        for a in alpha:\n",
    "            ridge = Ridge(fit_intercept=True, alpha=a)\n",
    "            ridge.fit(myTrain,myVal)\n",
    "            p = ridge.predict(myTrain)\n",
    "            err = p-myVal\n",
    "            total_error = np.dot(err,err)\n",
    "            rmse_train = np.sqrt(total_error/len(p))\n",
    "    \n",
    "            splits =kf(n_splits=10, shuffle=True, random_state=42)\n",
    "            xval_err = 0\n",
    "            for train,test in splits.split(myTrain):\n",
    "                ridge.fit(myTrain.ix[train],myVal.ix[train])\n",
    "                p = ridge.predict(myTrain.ix[test])\n",
    "                e = p-myVal.ix[test]\n",
    "                xval_err += np.dot(e,e)\n",
    "            rmse_10cv = np.sqrt(xval_err/len(myTrain))     \n",
    "    \n",
    "            t_rmse = np.append(t_rmse, [rmse_train])\n",
    "            cv_rmse = np.append(cv_rmse, [rmse_10cv])\n",
    "            print('{:.3f}\\t {:.4f}\\t\\t {:.4f}'.format(a,rmse_train,rmse_10cv))\n",
    "        input(\"Press Any Key\")\n",
    "        \n",
    "    plt.plot(alpha, t_rmse, label='RMSE-Train')\n",
    "    plt.plot(alpha, cv_rmse, label='RMSE_CV')\n",
    "    plt.legend( ('RMSE-Train', 'RMSE_XCV') )\n",
    "    plt.ylabel('RMSE')\n",
    "    plt.xlabel('Alpha')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RidgeCV 55.0821369773\n",
      "diabetes RidgeCV 55.0821369773\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'diabetes': {1: 'RidgeCV',\n",
       "  2: 42.149985571753987,\n",
       "  3: array([ -1.22670339e-01,  -2.20769374e+01,   1.30079387e+02,\n",
       "           7.67756543e+01,  -3.31633307e+01,  -1.55024577e+01,\n",
       "          -5.11710043e+01,   3.12611041e+01,   1.21081338e+02,\n",
       "           2.23936136e+01]),\n",
       "  4: array([  8.84555218e-01,   2.58290183e-10,   3.10999787e+56,\n",
       "           2.20415951e+33,   3.95683972e-15,   1.85083701e-07,\n",
       "           5.98019288e-23,   3.77159480e+13,   3.84553602e+52,\n",
       "           5.31401555e+09]),\n",
       "  5: array([-2760.46336984, -2953.66350592, -2466.89467959, -2996.86314214,\n",
       "         -3571.9536877 , -3027.32368837, -2396.92247752, -3255.52262074,\n",
       "         -3379.76346036, -3531.04750765]),\n",
       "  6: 55.082136977258955}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DS.runBestRegressionModelKFold([0],[DS.Regs[1]],[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-linearity and Ridge Regression\n",
    "\n",
    "### Motivation:\n",
    "\n",
    "Adding Nonlinearing or noise to the data increases the variability by which the linear regression effectiveness gets worse and we need regularization through ridge regression.\n",
    "\n",
    "\n",
    "**Validation using kfold and cross-validation for alpha and the data using synthesis dataset adding non-linearity**\n",
    "\n",
    "DS.bestRegressionSynthesisRidge:[a relative link](DS.py)\n",
    "\n",
    "\n",
    "Code:\n",
    "\n",
    "def bestRegressionSynthesis(Regs=[],names=[]): \n",
    "    \n",
    "    np.random.seed(42)\n",
    "    m = 100\n",
    "    X =  pre.scale(100* np.random.rand(m, 1) - 3)\n",
    "    y =  0.5 * X**2 + X + 2 + np.random.randn(m, 1)\n",
    "    \n",
    "    X_train, X_val, y_train, y_val = tts(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    infinity = float(\"inf\")\n",
    "    index=-1\n",
    "    count =-1\n",
    "    \n",
    "    for reg in Regs:\n",
    "        count = count +1\n",
    "        reg.fit(X_train, y_train.ravel())\n",
    "        pred=reg.predict(X_val)\n",
    "        meanSquareRootError=np.sqrt(mse(pred,y_val.ravel()))\n",
    "        print(regsNames[names[count]],meanSquareRootError)\n",
    "        if (meanSquareRootError < infinity):\n",
    "            infinity = meanSquareRootError\n",
    "            index = count\n",
    "            L1,L2,L3,L4,L5= reg.intercept_,reg.coef_,pred, np.exp(reg.coef_), meanSquareRootError\n",
    "    return regsNames[names[index]],L1,L2,L3,L4,L5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearRegression 0.957258936664\n",
      "RidgeCV 0.952268271599\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('RidgeCV',\n",
       " 2.4379271130332341,\n",
       " array([ 0.83353404]),\n",
       " array([ 1.29287429,  3.63373631,  3.2885425 ,  2.97956286,  1.8426204 ,\n",
       "         2.35336725,  1.93657565,  3.54440141,  1.17185947,  2.16860213,\n",
       "         2.33025492,  2.82474585,  3.41024106,  3.78596472,  1.45067195,\n",
       "         1.55324341,  3.28579852,  1.32240382,  3.45806033,  1.594091  ]),\n",
       " array([ 2.30143775]),\n",
       " 0.95226827159859306)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DS.bestRegressionSynthesis([DS.Regs[0],DS.Regs[1]],[0,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Lasso Regression](https://goo.gl/qZTDGP)\n",
    "\n",
    "### [Least Absolute and Selection Operator Regression (LASSO)](http://scikit-learn.org/stable/modules/linear_model.html#lasso)\n",
    "\n",
    "LASSO combines regularization with feature selection.\n",
    "\n",
    "The cost-function for LASSO is provided below\n",
    "\n",
    "\\begin{equation}\n",
    "J(\\Theta) = \\text{MSE}(\\Theta) + \\alpha \\sum_{i=1}^n |\\Theta_i|\n",
    "\\end{equation}\n",
    "\n",
    "### How does this cost function differ from the cost function for Ridge Regression?\n",
    "\n",
    "* Again, $\\alpha$ is a hyperparameter. \n",
    "* Again, we need to cross-validation to estimate $\\alpha$ and another one to report the model performance as described above.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LASSO with Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class Lasso in module sklearn.linear_model.coordinate_descent:\n",
      "\n",
      "class Lasso(ElasticNet)\n",
      " |  Linear Model trained with L1 prior as regularizer (aka the Lasso)\n",
      " |  \n",
      " |  The optimization objective for Lasso is::\n",
      " |  \n",
      " |      (1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1\n",
      " |  \n",
      " |  Technically the Lasso model is optimizing the same objective function as\n",
      " |  the Elastic Net with ``l1_ratio=1.0`` (no L2 penalty).\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <lasso>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  alpha : float, optional\n",
      " |      Constant that multiplies the L1 term. Defaults to 1.0.\n",
      " |      ``alpha = 0`` is equivalent to an ordinary least square, solved\n",
      " |      by the :class:`LinearRegression` object. For numerical\n",
      " |      reasons, using ``alpha = 0`` with the ``Lasso`` object is not advised.\n",
      " |      Given this, you should use the :class:`LinearRegression` object.\n",
      " |  \n",
      " |  fit_intercept : boolean\n",
      " |      whether to calculate the intercept for this model. If set\n",
      " |      to false, no intercept will be used in calculations\n",
      " |      (e.g. data is expected to be already centered).\n",
      " |  \n",
      " |  normalize : boolean, optional, default False\n",
      " |      If ``True``, the regressors X will be normalized before regression.\n",
      " |      This parameter is ignored when ``fit_intercept`` is set to ``False``.\n",
      " |      When the regressors are normalized, note that this makes the\n",
      " |      hyperparameters learnt more robust and almost independent of the number\n",
      " |      of samples. The same property is not valid for standardized data.\n",
      " |      However, if you wish to standardize, please use\n",
      " |      :class:`preprocessing.StandardScaler` before calling ``fit`` on an estimator\n",
      " |      with ``normalize=False``.\n",
      " |  \n",
      " |  copy_X : boolean, optional, default True\n",
      " |      If ``True``, X will be copied; else, it may be overwritten.\n",
      " |  \n",
      " |  precompute : True | False | array-like, default=False\n",
      " |      Whether to use a precomputed Gram matrix to speed up\n",
      " |      calculations. If set to ``'auto'`` let us decide. The Gram\n",
      " |      matrix can also be passed as argument. For sparse input\n",
      " |      this option is always ``True`` to preserve sparsity.\n",
      " |  \n",
      " |  max_iter : int, optional\n",
      " |      The maximum number of iterations\n",
      " |  \n",
      " |  tol : float, optional\n",
      " |      The tolerance for the optimization: if the updates are\n",
      " |      smaller than ``tol``, the optimization code checks the\n",
      " |      dual gap for optimality and continues until it is smaller\n",
      " |      than ``tol``.\n",
      " |  \n",
      " |  warm_start : bool, optional\n",
      " |      When set to True, reuse the solution of the previous call to fit as\n",
      " |      initialization, otherwise, just erase the previous solution.\n",
      " |  \n",
      " |  positive : bool, optional\n",
      " |      When set to ``True``, forces the coefficients to be positive.\n",
      " |  \n",
      " |  selection : str, default 'cyclic'\n",
      " |      If set to 'random', a random coefficient is updated every iteration\n",
      " |      rather than looping over features sequentially by default. This\n",
      " |      (setting to 'random') often leads to significantly faster convergence\n",
      " |      especially when tol is higher than 1e-4.\n",
      " |  \n",
      " |  random_state : int, RandomState instance, or None (default)\n",
      " |      The seed of the pseudo random number generator that selects\n",
      " |      a random feature to update. Useful only when selection is set to\n",
      " |      'random'.\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  coef_ : array, shape (n_features,) | (n_targets, n_features)\n",
      " |      parameter vector (w in the cost function formula)\n",
      " |  \n",
      " |  sparse_coef_ : scipy.sparse matrix, shape (n_features, 1) |             (n_targets, n_features)\n",
      " |      ``sparse_coef_`` is a readonly property derived from ``coef_``\n",
      " |  \n",
      " |  intercept_ : float | array, shape (n_targets,)\n",
      " |      independent term in decision function.\n",
      " |  \n",
      " |  n_iter_ : int | array-like, shape (n_targets,)\n",
      " |      number of iterations run by the coordinate descent solver to reach\n",
      " |      the specified tolerance.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> from sklearn import linear_model\n",
      " |  >>> clf = linear_model.Lasso(alpha=0.1)\n",
      " |  >>> clf.fit([[0,0], [1, 1], [2, 2]], [0, 1, 2])\n",
      " |  Lasso(alpha=0.1, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      " |     normalize=False, positive=False, precompute=False, random_state=None,\n",
      " |     selection='cyclic', tol=0.0001, warm_start=False)\n",
      " |  >>> print(clf.coef_)\n",
      " |  [ 0.85  0.  ]\n",
      " |  >>> print(clf.intercept_)\n",
      " |  0.15\n",
      " |  \n",
      " |  See also\n",
      " |  --------\n",
      " |  lars_path\n",
      " |  lasso_path\n",
      " |  LassoLars\n",
      " |  LassoCV\n",
      " |  LassoLarsCV\n",
      " |  sklearn.decomposition.sparse_encode\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  The algorithm used to fit the model is coordinate descent.\n",
      " |  \n",
      " |  To avoid unnecessary memory duplication the X argument of the fit method\n",
      " |  should be directly passed as a Fortran-contiguous numpy array.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      Lasso\n",
      " |      ElasticNet\n",
      " |      sklearn.linear_model.base.LinearModel\n",
      " |      abc.NewBase\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      sklearn.base.RegressorMixin\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, alpha=1.0, fit_intercept=True, normalize=False, precompute=False, copy_X=True, max_iter=1000, tol=0.0001, warm_start=False, positive=False, random_state=None, selection='cyclic')\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods defined here:\n",
      " |  \n",
      " |  path = enet_path(X, y, l1_ratio=0.5, eps=0.001, n_alphas=100, alphas=None, precompute='auto', Xy=None, copy_X=True, coef_init=None, verbose=False, return_n_iter=False, positive=False, check_input=True, **params)\n",
      " |      Compute elastic net path with coordinate descent\n",
      " |      \n",
      " |      The elastic net optimization function varies for mono and multi-outputs.\n",
      " |      \n",
      " |      For mono-output tasks it is::\n",
      " |      \n",
      " |          1 / (2 * n_samples) * ||y - Xw||^2_2\n",
      " |          + alpha * l1_ratio * ||w||_1\n",
      " |          + 0.5 * alpha * (1 - l1_ratio) * ||w||^2_2\n",
      " |      \n",
      " |      For multi-output tasks it is::\n",
      " |      \n",
      " |          (1 / (2 * n_samples)) * ||Y - XW||^Fro_2\n",
      " |          + alpha * l1_ratio * ||W||_21\n",
      " |          + 0.5 * alpha * (1 - l1_ratio) * ||W||_Fro^2\n",
      " |      \n",
      " |      Where::\n",
      " |      \n",
      " |          ||W||_21 = \\sum_i \\sqrt{\\sum_j w_{ij}^2}\n",
      " |      \n",
      " |      i.e. the sum of norm of each row.\n",
      " |      \n",
      " |      Read more in the :ref:`User Guide <elastic_net>`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like}, shape (n_samples, n_features)\n",
      " |          Training data. Pass directly as Fortran-contiguous data to avoid\n",
      " |          unnecessary memory duplication. If ``y`` is mono-output then ``X``\n",
      " |          can be sparse.\n",
      " |      \n",
      " |      y : ndarray, shape (n_samples,) or (n_samples, n_outputs)\n",
      " |          Target values\n",
      " |      \n",
      " |      l1_ratio : float, optional\n",
      " |          float between 0 and 1 passed to elastic net (scaling between\n",
      " |          l1 and l2 penalties). ``l1_ratio=1`` corresponds to the Lasso\n",
      " |      \n",
      " |      eps : float\n",
      " |          Length of the path. ``eps=1e-3`` means that\n",
      " |          ``alpha_min / alpha_max = 1e-3``\n",
      " |      \n",
      " |      n_alphas : int, optional\n",
      " |          Number of alphas along the regularization path\n",
      " |      \n",
      " |      alphas : ndarray, optional\n",
      " |          List of alphas where to compute the models.\n",
      " |          If None alphas are set automatically\n",
      " |      \n",
      " |      precompute : True | False | 'auto' | array-like\n",
      " |          Whether to use a precomputed Gram matrix to speed up\n",
      " |          calculations. If set to ``'auto'`` let us decide. The Gram\n",
      " |          matrix can also be passed as argument.\n",
      " |      \n",
      " |      Xy : array-like, optional\n",
      " |          Xy = np.dot(X.T, y) that can be precomputed. It is useful\n",
      " |          only when the Gram matrix is precomputed.\n",
      " |      \n",
      " |      copy_X : boolean, optional, default True\n",
      " |          If ``True``, X will be copied; else, it may be overwritten.\n",
      " |      \n",
      " |      coef_init : array, shape (n_features, ) | None\n",
      " |          The initial values of the coefficients.\n",
      " |      \n",
      " |      verbose : bool or integer\n",
      " |          Amount of verbosity.\n",
      " |      \n",
      " |      params : kwargs\n",
      " |          keyword arguments passed to the coordinate descent solver.\n",
      " |      \n",
      " |      return_n_iter : bool\n",
      " |          whether to return the number of iterations or not.\n",
      " |      \n",
      " |      positive : bool, default False\n",
      " |          If set to True, forces coefficients to be positive.\n",
      " |      \n",
      " |      check_input : bool, default True\n",
      " |          Skip input validation checks, including the Gram matrix when provided\n",
      " |          assuming there are handled by the caller when check_input=False.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      alphas : array, shape (n_alphas,)\n",
      " |          The alphas along the path where models are computed.\n",
      " |      \n",
      " |      coefs : array, shape (n_features, n_alphas) or             (n_outputs, n_features, n_alphas)\n",
      " |          Coefficients along the path.\n",
      " |      \n",
      " |      dual_gaps : array, shape (n_alphas,)\n",
      " |          The dual gaps at the end of the optimization for each alpha.\n",
      " |      \n",
      " |      n_iters : array-like, shape (n_alphas,)\n",
      " |          The number of iterations taken by the coordinate descent optimizer to\n",
      " |          reach the specified tolerance for each alpha.\n",
      " |          (Is returned when ``return_n_iter`` is set to True).\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      See examples/linear_model/plot_lasso_coordinate_descent_path.py for an example.\n",
      " |      \n",
      " |      See also\n",
      " |      --------\n",
      " |      MultiTaskElasticNet\n",
      " |      MultiTaskElasticNetCV\n",
      " |      ElasticNet\n",
      " |      ElasticNetCV\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from ElasticNet:\n",
      " |  \n",
      " |  decision_function(*args, **kwargs)\n",
      " |      DEPRECATED:  and will be removed in 0.19\n",
      " |      \n",
      " |      Decision function of the linear model\n",
      " |      \n",
      " |              Parameters\n",
      " |              ----------\n",
      " |              X : numpy array or scipy.sparse matrix of shape (n_samples, n_features)\n",
      " |      \n",
      " |              Returns\n",
      " |              -------\n",
      " |              T : array, shape (n_samples,)\n",
      " |                  The predicted decision function\n",
      " |  \n",
      " |  fit(self, X, y, check_input=True)\n",
      " |      Fit model with coordinate descent.\n",
      " |      \n",
      " |      Parameters\n",
      " |      -----------\n",
      " |      X : ndarray or scipy.sparse matrix, (n_samples, n_features)\n",
      " |          Data\n",
      " |      \n",
      " |      y : ndarray, shape (n_samples,) or (n_samples, n_targets)\n",
      " |          Target\n",
      " |      \n",
      " |      check_input : boolean, (default=True)\n",
      " |          Allow to bypass several input checking.\n",
      " |          Don't use this parameter unless you know what you do.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      \n",
      " |      Coordinate descent is an algorithm that considers each column of\n",
      " |      data at a time hence it will automatically convert the X input\n",
      " |      as a Fortran-contiguous numpy array if necessary.\n",
      " |      \n",
      " |      To avoid memory re-allocation it is advised to allocate the\n",
      " |      initial data in memory directly using that format.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from ElasticNet:\n",
      " |  \n",
      " |  sparse_coef_\n",
      " |      sparse representation of the fitted ``coef_``\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.linear_model.base.LinearModel:\n",
      " |  \n",
      " |  predict(self, X)\n",
      " |      Predict using the linear model\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix}, shape = (n_samples, n_features)\n",
      " |          Samples.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      C : array, shape = (n_samples,)\n",
      " |          Returns predicted values.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : boolean, optional\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : mapping of string to any\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as pipelines). The latter have parameters of the form\n",
      " |      ``<component>__<parameter>`` so that it's possible to update each\n",
      " |      component of a nested object.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.RegressorMixin:\n",
      " |  \n",
      " |  score(self, X, y, sample_weight=None)\n",
      " |      Returns the coefficient of determination R^2 of the prediction.\n",
      " |      \n",
      " |      The coefficient R^2 is defined as (1 - u/v), where u is the regression\n",
      " |      sum of squares ((y_true - y_pred) ** 2).sum() and v is the residual\n",
      " |      sum of squares ((y_true - y_true.mean()) ** 2).sum().\n",
      " |      Best possible score is 1.0 and it can be negative (because the\n",
      " |      model can be arbitrarily worse). A constant model that always\n",
      " |      predicts the expected value of y, disregarding the input features,\n",
      " |      would get a R^2 score of 0.0.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape = (n_samples, n_features)\n",
      " |          Test samples.\n",
      " |      \n",
      " |      y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n",
      " |          True values for X.\n",
      " |      \n",
      " |      sample_weight : array-like, shape = [n_samples], optional\n",
      " |          Sample weights.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          R^2 of self.predict(X) wrt. y.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(linear_model.Lasso)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Elastic Net](https://en.wikipedia.org/wiki/Elastic_net_regularization)\n",
    "\n",
    "\n",
    "[Elastic net](http://scikit-learn.org/stable/modules/linear_model.html#elastic-net) is a middle ground between Ridge and LASSO. The cost function for Elastic Net is\n",
    "\n",
    "\\begin{equation}\n",
    "J(\\Theta) = \\text{MSE}(\\Theta) + r\\alpha \\sum_{i=1}^n |\\Theta_i| + (1-r)\\frac{\\alpha}{2} \\sum_{i=1}^n |\\Theta_i|^2\n",
    "\\end{equation}\n",
    "\n",
    "#### The regularization is the weighted average of the Ridge and LASSO regularization (notice the $r$ and $(1-r)$ terms\n",
    "\n",
    "* $\\alpha$ and $r$ are hyperparamters\n",
    "* Again, we need one cross-validation to estimate $\\alpha$ and $r$ and another cross-validation to optimize the $\\Theta_i$'s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elastic Net with Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class ElasticNet in module sklearn.linear_model.coordinate_descent:\n",
      "\n",
      "class ElasticNet(sklearn.linear_model.base.LinearModel, sklearn.base.RegressorMixin)\n",
      " |  Linear regression with combined L1 and L2 priors as regularizer.\n",
      " |  \n",
      " |  Minimizes the objective function::\n",
      " |  \n",
      " |          1 / (2 * n_samples) * ||y - Xw||^2_2\n",
      " |          + alpha * l1_ratio * ||w||_1\n",
      " |          + 0.5 * alpha * (1 - l1_ratio) * ||w||^2_2\n",
      " |  \n",
      " |  If you are interested in controlling the L1 and L2 penalty\n",
      " |  separately, keep in mind that this is equivalent to::\n",
      " |  \n",
      " |          a * L1 + b * L2\n",
      " |  \n",
      " |  where::\n",
      " |  \n",
      " |          alpha = a + b and l1_ratio = a / (a + b)\n",
      " |  \n",
      " |  The parameter l1_ratio corresponds to alpha in the glmnet R package while\n",
      " |  alpha corresponds to the lambda parameter in glmnet. Specifically, l1_ratio\n",
      " |  = 1 is the lasso penalty. Currently, l1_ratio <= 0.01 is not reliable,\n",
      " |  unless you supply your own sequence of alpha.\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <elastic_net>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  alpha : float, optional\n",
      " |      Constant that multiplies the penalty terms. Defaults to 1.0.\n",
      " |      See the notes for the exact mathematical meaning of this\n",
      " |      parameter.``alpha = 0`` is equivalent to an ordinary least square, solved\n",
      " |      by the :class:`LinearRegression` object. For numerical\n",
      " |      reasons, using ``alpha = 0`` with the ``Lasso`` object is not advised.\n",
      " |      Given this, you should use the :class:`LinearRegression` object.\n",
      " |  \n",
      " |  l1_ratio : float\n",
      " |      The ElasticNet mixing parameter, with ``0 <= l1_ratio <= 1``. For\n",
      " |      ``l1_ratio = 0`` the penalty is an L2 penalty. ``For l1_ratio = 1`` it\n",
      " |      is an L1 penalty.  For ``0 < l1_ratio < 1``, the penalty is a\n",
      " |      combination of L1 and L2.\n",
      " |  \n",
      " |  fit_intercept : bool\n",
      " |      Whether the intercept should be estimated or not. If ``False``, the\n",
      " |      data is assumed to be already centered.\n",
      " |  \n",
      " |  normalize : boolean, optional, default False\n",
      " |      If ``True``, the regressors X will be normalized before regression.\n",
      " |      This parameter is ignored when ``fit_intercept`` is set to ``False``.\n",
      " |      When the regressors are normalized, note that this makes the\n",
      " |      hyperparameters learnt more robust and almost independent of the number\n",
      " |      of samples. The same property is not valid for standardized data.\n",
      " |      However, if you wish to standardize, please use\n",
      " |      :class:`preprocessing.StandardScaler` before calling ``fit`` on an estimator\n",
      " |      with ``normalize=False``.\n",
      " |  \n",
      " |  precompute : True | False | array-like\n",
      " |      Whether to use a precomputed Gram matrix to speed up\n",
      " |      calculations. The Gram matrix can also be passed as argument.\n",
      " |      For sparse input this option is always ``True`` to preserve sparsity.\n",
      " |  \n",
      " |  max_iter : int, optional\n",
      " |      The maximum number of iterations\n",
      " |  \n",
      " |  copy_X : boolean, optional, default True\n",
      " |      If ``True``, X will be copied; else, it may be overwritten.\n",
      " |  \n",
      " |  tol : float, optional\n",
      " |      The tolerance for the optimization: if the updates are\n",
      " |      smaller than ``tol``, the optimization code checks the\n",
      " |      dual gap for optimality and continues until it is smaller\n",
      " |      than ``tol``.\n",
      " |  \n",
      " |  warm_start : bool, optional\n",
      " |      When set to ``True``, reuse the solution of the previous call to fit as\n",
      " |      initialization, otherwise, just erase the previous solution.\n",
      " |  \n",
      " |  positive : bool, optional\n",
      " |      When set to ``True``, forces the coefficients to be positive.\n",
      " |  \n",
      " |  selection : str, default 'cyclic'\n",
      " |      If set to 'random', a random coefficient is updated every iteration\n",
      " |      rather than looping over features sequentially by default. This\n",
      " |      (setting to 'random') often leads to significantly faster convergence\n",
      " |      especially when tol is higher than 1e-4.\n",
      " |  \n",
      " |  random_state : int, RandomState instance, or None (default)\n",
      " |      The seed of the pseudo random number generator that selects\n",
      " |      a random feature to update. Useful only when selection is set to\n",
      " |      'random'.\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  coef_ : array, shape (n_features,) | (n_targets, n_features)\n",
      " |      parameter vector (w in the cost function formula)\n",
      " |  \n",
      " |  sparse_coef_ : scipy.sparse matrix, shape (n_features, 1) |             (n_targets, n_features)\n",
      " |      ``sparse_coef_`` is a readonly property derived from ``coef_``\n",
      " |  \n",
      " |  intercept_ : float | array, shape (n_targets,)\n",
      " |      independent term in decision function.\n",
      " |  \n",
      " |  n_iter_ : array-like, shape (n_targets,)\n",
      " |      number of iterations run by the coordinate descent solver to reach\n",
      " |      the specified tolerance.\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  To avoid unnecessary memory duplication the X argument of the fit method\n",
      " |  should be directly passed as a Fortran-contiguous numpy array.\n",
      " |  \n",
      " |  See also\n",
      " |  --------\n",
      " |  SGDRegressor: implements elastic net regression with incremental training.\n",
      " |  SGDClassifier: implements logistic regression with elastic net penalty\n",
      " |      (``SGDClassifier(loss=\"log\", penalty=\"elasticnet\")``).\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      ElasticNet\n",
      " |      sklearn.linear_model.base.LinearModel\n",
      " |      abc.NewBase\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      sklearn.base.RegressorMixin\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, alpha=1.0, l1_ratio=0.5, fit_intercept=True, normalize=False, precompute=False, max_iter=1000, copy_X=True, tol=0.0001, warm_start=False, positive=False, random_state=None, selection='cyclic')\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  decision_function(*args, **kwargs)\n",
      " |      DEPRECATED:  and will be removed in 0.19\n",
      " |      \n",
      " |      Decision function of the linear model\n",
      " |      \n",
      " |              Parameters\n",
      " |              ----------\n",
      " |              X : numpy array or scipy.sparse matrix of shape (n_samples, n_features)\n",
      " |      \n",
      " |              Returns\n",
      " |              -------\n",
      " |              T : array, shape (n_samples,)\n",
      " |                  The predicted decision function\n",
      " |  \n",
      " |  fit(self, X, y, check_input=True)\n",
      " |      Fit model with coordinate descent.\n",
      " |      \n",
      " |      Parameters\n",
      " |      -----------\n",
      " |      X : ndarray or scipy.sparse matrix, (n_samples, n_features)\n",
      " |          Data\n",
      " |      \n",
      " |      y : ndarray, shape (n_samples,) or (n_samples, n_targets)\n",
      " |          Target\n",
      " |      \n",
      " |      check_input : boolean, (default=True)\n",
      " |          Allow to bypass several input checking.\n",
      " |          Don't use this parameter unless you know what you do.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      \n",
      " |      Coordinate descent is an algorithm that considers each column of\n",
      " |      data at a time hence it will automatically convert the X input\n",
      " |      as a Fortran-contiguous numpy array if necessary.\n",
      " |      \n",
      " |      To avoid memory re-allocation it is advised to allocate the\n",
      " |      initial data in memory directly using that format.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods defined here:\n",
      " |  \n",
      " |  path = enet_path(X, y, l1_ratio=0.5, eps=0.001, n_alphas=100, alphas=None, precompute='auto', Xy=None, copy_X=True, coef_init=None, verbose=False, return_n_iter=False, positive=False, check_input=True, **params)\n",
      " |      Compute elastic net path with coordinate descent\n",
      " |      \n",
      " |      The elastic net optimization function varies for mono and multi-outputs.\n",
      " |      \n",
      " |      For mono-output tasks it is::\n",
      " |      \n",
      " |          1 / (2 * n_samples) * ||y - Xw||^2_2\n",
      " |          + alpha * l1_ratio * ||w||_1\n",
      " |          + 0.5 * alpha * (1 - l1_ratio) * ||w||^2_2\n",
      " |      \n",
      " |      For multi-output tasks it is::\n",
      " |      \n",
      " |          (1 / (2 * n_samples)) * ||Y - XW||^Fro_2\n",
      " |          + alpha * l1_ratio * ||W||_21\n",
      " |          + 0.5 * alpha * (1 - l1_ratio) * ||W||_Fro^2\n",
      " |      \n",
      " |      Where::\n",
      " |      \n",
      " |          ||W||_21 = \\sum_i \\sqrt{\\sum_j w_{ij}^2}\n",
      " |      \n",
      " |      i.e. the sum of norm of each row.\n",
      " |      \n",
      " |      Read more in the :ref:`User Guide <elastic_net>`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like}, shape (n_samples, n_features)\n",
      " |          Training data. Pass directly as Fortran-contiguous data to avoid\n",
      " |          unnecessary memory duplication. If ``y`` is mono-output then ``X``\n",
      " |          can be sparse.\n",
      " |      \n",
      " |      y : ndarray, shape (n_samples,) or (n_samples, n_outputs)\n",
      " |          Target values\n",
      " |      \n",
      " |      l1_ratio : float, optional\n",
      " |          float between 0 and 1 passed to elastic net (scaling between\n",
      " |          l1 and l2 penalties). ``l1_ratio=1`` corresponds to the Lasso\n",
      " |      \n",
      " |      eps : float\n",
      " |          Length of the path. ``eps=1e-3`` means that\n",
      " |          ``alpha_min / alpha_max = 1e-3``\n",
      " |      \n",
      " |      n_alphas : int, optional\n",
      " |          Number of alphas along the regularization path\n",
      " |      \n",
      " |      alphas : ndarray, optional\n",
      " |          List of alphas where to compute the models.\n",
      " |          If None alphas are set automatically\n",
      " |      \n",
      " |      precompute : True | False | 'auto' | array-like\n",
      " |          Whether to use a precomputed Gram matrix to speed up\n",
      " |          calculations. If set to ``'auto'`` let us decide. The Gram\n",
      " |          matrix can also be passed as argument.\n",
      " |      \n",
      " |      Xy : array-like, optional\n",
      " |          Xy = np.dot(X.T, y) that can be precomputed. It is useful\n",
      " |          only when the Gram matrix is precomputed.\n",
      " |      \n",
      " |      copy_X : boolean, optional, default True\n",
      " |          If ``True``, X will be copied; else, it may be overwritten.\n",
      " |      \n",
      " |      coef_init : array, shape (n_features, ) | None\n",
      " |          The initial values of the coefficients.\n",
      " |      \n",
      " |      verbose : bool or integer\n",
      " |          Amount of verbosity.\n",
      " |      \n",
      " |      params : kwargs\n",
      " |          keyword arguments passed to the coordinate descent solver.\n",
      " |      \n",
      " |      return_n_iter : bool\n",
      " |          whether to return the number of iterations or not.\n",
      " |      \n",
      " |      positive : bool, default False\n",
      " |          If set to True, forces coefficients to be positive.\n",
      " |      \n",
      " |      check_input : bool, default True\n",
      " |          Skip input validation checks, including the Gram matrix when provided\n",
      " |          assuming there are handled by the caller when check_input=False.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      alphas : array, shape (n_alphas,)\n",
      " |          The alphas along the path where models are computed.\n",
      " |      \n",
      " |      coefs : array, shape (n_features, n_alphas) or             (n_outputs, n_features, n_alphas)\n",
      " |          Coefficients along the path.\n",
      " |      \n",
      " |      dual_gaps : array, shape (n_alphas,)\n",
      " |          The dual gaps at the end of the optimization for each alpha.\n",
      " |      \n",
      " |      n_iters : array-like, shape (n_alphas,)\n",
      " |          The number of iterations taken by the coordinate descent optimizer to\n",
      " |          reach the specified tolerance for each alpha.\n",
      " |          (Is returned when ``return_n_iter`` is set to True).\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      See examples/linear_model/plot_lasso_coordinate_descent_path.py for an example.\n",
      " |      \n",
      " |      See also\n",
      " |      --------\n",
      " |      MultiTaskElasticNet\n",
      " |      MultiTaskElasticNetCV\n",
      " |      ElasticNet\n",
      " |      ElasticNetCV\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  sparse_coef_\n",
      " |      sparse representation of the fitted ``coef_``\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.linear_model.base.LinearModel:\n",
      " |  \n",
      " |  predict(self, X)\n",
      " |      Predict using the linear model\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix}, shape = (n_samples, n_features)\n",
      " |          Samples.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      C : array, shape = (n_samples,)\n",
      " |          Returns predicted values.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : boolean, optional\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : mapping of string to any\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as pipelines). The latter have parameters of the form\n",
      " |      ``<component>__<parameter>`` so that it's possible to update each\n",
      " |      component of a nested object.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.RegressorMixin:\n",
      " |  \n",
      " |  score(self, X, y, sample_weight=None)\n",
      " |      Returns the coefficient of determination R^2 of the prediction.\n",
      " |      \n",
      " |      The coefficient R^2 is defined as (1 - u/v), where u is the regression\n",
      " |      sum of squares ((y_true - y_pred) ** 2).sum() and v is the residual\n",
      " |      sum of squares ((y_true - y_true.mean()) ** 2).sum().\n",
      " |      Best possible score is 1.0 and it can be negative (because the\n",
      " |      model can be arbitrarily worse). A constant model that always\n",
      " |      predicts the expected value of y, disregarding the input features,\n",
      " |      would get a R^2 score of 0.0.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape = (n_samples, n_features)\n",
      " |          Test samples.\n",
      " |      \n",
      " |      y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n",
      " |          True values for X.\n",
      " |      \n",
      " |      sample_weight : array-like, shape = [n_samples], optional\n",
      " |          Sample weights.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          R^2 of self.predict(X) wrt. y.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(linear_model.ElasticNet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparisons\n",
    "\n",
    "** Which have better results ridge, Lasso, ElasticNet? Why? **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Exercise\n",
    "\n",
    "### Questions\n",
    "\n",
    "1. Use the functions in the DS.py to build models using Lasso and Elastic Net using both regression datasets.\n",
    "2. Compare the results (mainly MSE) with other regression models.\n",
    "3. Give some interpretations for your results.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
